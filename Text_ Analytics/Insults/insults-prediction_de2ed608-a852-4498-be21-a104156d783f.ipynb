{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Insults in Social Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pbs.twimg.com/media/CkEyfjKUUAURpd9.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.findall(r'\\b[a-z]+\\b', text)\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620171226Z</td>\n",
       "      <td>\"@SDL OK, but I would hope they'd sign him to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>20120503012628Z</td>\n",
       "      <td>\"Yeah and where are you now?\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment\n",
       "0       1  20120618192155Z                               \"You fuck your dad.\"\n",
       "1       0  20120528192215Z  \"i really don't understand your point.\\xa0 It ...\n",
       "2       0              NaN  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...\n",
       "3       0              NaN  \"listen if you dont wanna get married to a man...\n",
       "4       0  20120619094753Z  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...\n",
       "5       0  20120620171226Z  \"@SDL OK, but I would hope they'd sign him to ...\n",
       "6       0  20120503012628Z                      \"Yeah and where are you now?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pd.read_csv('train.csv')\n",
    "training_data.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['cleaned_comment'] = training_data['Comment'].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>cleaned_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "      <td>you fuck your dad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "      <td>i really don t understand your point it seems ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "      <td>a of canadians can and has been wrong before n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "      <td>listen if you dont wanna get married to a man ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "      <td>c b xu bi t xecnh c ho kh nc ng d ng cu xed ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620171226Z</td>\n",
       "      <td>\"@SDL OK, but I would hope they'd sign him to ...</td>\n",
       "      <td>sdl ok but i would hope they d sign him to a o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>20120503012628Z</td>\n",
       "      <td>\"Yeah and where are you now?\"</td>\n",
       "      <td>yeah and where are you now</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment  \\\n",
       "0       1  20120618192155Z                               \"You fuck your dad.\"   \n",
       "1       0  20120528192215Z  \"i really don't understand your point.\\xa0 It ...   \n",
       "2       0              NaN  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...   \n",
       "3       0              NaN  \"listen if you dont wanna get married to a man...   \n",
       "4       0  20120619094753Z  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...   \n",
       "5       0  20120620171226Z  \"@SDL OK, but I would hope they'd sign him to ...   \n",
       "6       0  20120503012628Z                      \"Yeah and where are you now?\"   \n",
       "\n",
       "                                     cleaned_comment  \n",
       "0                                  you fuck your dad  \n",
       "1  i really don t understand your point it seems ...  \n",
       "2  a of canadians can and has been wrong before n...  \n",
       "3  listen if you dont wanna get married to a man ...  \n",
       "4  c b xu bi t xecnh c ho kh nc ng d ng cu xed ch...  \n",
       "5  sdl ok but i would hope they d sign him to a o...  \n",
       "6                         yeah and where are you now  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=50000, min_df=1,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,3), stop_words='english', max_features=50000)\n",
    "count_vectorizer.fit(training_data['cleaned_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fuck': 3185,\n",
       " 'dad': 1781,\n",
       " 'really': 30107,\n",
       " 'don': 2254,\n",
       " 'understand': 40540,\n",
       " 'point': 23882,\n",
       " 'mixing': 9152,\n",
       " 'apples': 301,\n",
       " 'oranges': 18837,\n",
       " 'really don': 30172,\n",
       " 'don understand': 2307,\n",
       " 'understand point': 40614,\n",
       " 'point mixing': 23951,\n",
       " 'mixing apples': 9153,\n",
       " 'apples oranges': 302,\n",
       " 'really don understand': 30178,\n",
       " 'understand point mixing': 40616,\n",
       " 'point mixing apples': 23952,\n",
       " 'mixing apples oranges': 9154,\n",
       " 'canadians': 1045,\n",
       " 'wrong': 48282,\n",
       " 'nunless': 16508,\n",
       " 'idea': 4038,\n",
       " 'proof': 27049,\n",
       " 'perfect': 21899,\n",
       " 'chances': 1166,\n",
       " 'inadvertently': 4144,\n",
       " 'kill': 4603,\n",
       " 'son': 33852,\n",
       " 'daughter': 1822,\n",
       " 'breaks': 898,\n",
       " 'regard': 31109,\n",
       " 'damage': 1792,\n",
       " 'like': 4916,\n",
       " 'wartime': 44263,\n",
       " 'sorry': 33866,\n",
       " 'mail': 5274,\n",
       " 'wrong nunless': 48325,\n",
       " 'nunless supportive': 16509,\n",
       " 'proof perfect': 27066,\n",
       " 'perfect chances': 21900,\n",
       " 'regard collateral': 31114,\n",
       " 'wartime sorry': 44264,\n",
       " 'wrong nunless supportive': 48326,\n",
       " 'nunless supportive idea': 16510,\n",
       " 'proof perfect chances': 27067,\n",
       " 'perfect chances inadvertently': 21901,\n",
       " 'regard collateral damage': 31115,\n",
       " 'wartime sorry cheques': 44265,\n",
       " 'listen': 5048,\n",
       " 'dont': 2322,\n",
       " 'wanna': 43615,\n",
       " 'married': 6523,\n",
       " 'man': 5843,\n",
       " 'women': 46877,\n",
       " 'bother': 860,\n",
       " 'gay': 3312,\n",
       " 'people': 21153,\n",
       " 'got': 3482,\n",
       " 'stay': 34028,\n",
       " 'lane': 4722,\n",
       " 'let': 4838,\n",
       " 'god': 3409,\n",
       " 'nice': 13940,\n",
       " 'quick': 28538,\n",
       " 'thought': 36611,\n",
       " 'wasnt': 44357,\n",
       " 'suppose': 34224,\n",
       " 'judge': 4429,\n",
       " 'dont wanna': 2333,\n",
       " 'wanna married': 43625,\n",
       " 'married man': 6528,\n",
       " 'man women': 6087,\n",
       " 'women dont': 46898,\n",
       " 'gay people': 3321,\n",
       " 'people got': 21407,\n",
       " 'got married': 3488,\n",
       " 'married stay': 6540,\n",
       " 'nice quick': 13989,\n",
       " 'quick judg': 28539,\n",
       " 'thought wasnt': 36706,\n",
       " 'wasnt suppose': 44362,\n",
       " 'wanna married man': 43626,\n",
       " 'married man women': 6529,\n",
       " 'man women dont': 6088,\n",
       " 'women dont bother': 46899,\n",
       " 'people got married': 21409,\n",
       " 'married stay lane': 6541,\n",
       " 'nice quick judg': 13990,\n",
       " 'quick judg like': 28540,\n",
       " 'thought wasnt suppose': 36707,\n",
       " 'wasnt suppose judge': 44363,\n",
       " 'xu': 48711,\n",
       " 'bi': 662,\n",
       " 'xecnh': 48632,\n",
       " 'ho': 3835,\n",
       " 'kh': 4583,\n",
       " 'nc': 11961,\n",
       " 'ng': 13399,\n",
       " 'cu': 1757,\n",
       " 'xed': 48641,\n",
       " 'chi': 1208,\n",
       " 'nh': 13517,\n",
       " 'gi': 3364,\n",
       " 'th': 35206,\n",
       " 'nr': 15632,\n",
       " 'xec': 48611,\n",
       " 'ch': 1150,\n",
       " 'xfang': 48681,\n",
       " 'ta': 34277,\n",
       " 'nai': 11139,\n",
       " 'xeau': 48602,\n",
       " 'khi': 4587,\n",
       " 'sau': 32082,\n",
       " 'tranh': 38591,\n",
       " 'nkh': 14540,\n",
       " 'xeam': 48578,\n",
       " 'xu bi': 48712,\n",
       " 'xecnh ho': 48635,\n",
       " 'ho kh': 3837,\n",
       " 'kh nc': 4584,\n",
       " 'nc ng': 11964,\n",
       " 'ng ng': 13409,\n",
       " 'ng cu': 13403,\n",
       " 'xed chi': 48642,\n",
       " 'nh ho': 13520,\n",
       " 'nc gi': 11962,\n",
       " 'th ho': 35209,\n",
       " 'nr cu': 15633,\n",
       " 'xec th': 48623,\n",
       " 'th xec': 35237,\n",
       " 'xec ch': 48614,\n",
       " 'xfang ta': 48682,\n",
       " 'nai xeau': 11140,\n",
       " 'xeau chu': 48605,\n",
       " 'ho xecnh': 3842,\n",
       " 'xecnh nh': 48639,\n",
       " 'nh khi': 13525,\n",
       " 'xecnh ch': 48633,\n",
       " 'ch th': 1152,\n",
       " 'th sau': 35222,\n",
       " 'tranh th': 38592,\n",
       " 'th nkh': 35212,\n",
       " 'nkh ch': 14541,\n",
       " 'kh th': 4585,\n",
       " 'th xeam': 35234,\n",
       " 'xu bi xecnh': 48713,\n",
       " 'xecnh ho kh': 48636,\n",
       " 'ho kh nc': 3838,\n",
       " 'nc ng ng': 11965,\n",
       " 'ng ng cu': 13410,\n",
       " 'ng cu xed': 13404,\n",
       " 'xed chi nh': 48643,\n",
       " 'nh ho kh': 13521,\n",
       " 'nc gi giang': 11963,\n",
       " 'th ho kh': 35210,\n",
       " 'nr cu xec': 15634,\n",
       " 'xec th xec': 48624,\n",
       " 'th xec ch': 35239,\n",
       " 'xec ch xfang': 48615,\n",
       " 'xfang ta bi': 48683,\n",
       " 'nai xeau chu': 11141,\n",
       " 'xeau chu ho': 48606,\n",
       " 'xecnh nh khi': 48640,\n",
       " 'nh khi ho': 13526,\n",
       " 'xecnh ch th': 48634,\n",
       " 'th sau chi': 35223,\n",
       " 'tranh th nkh': 38593,\n",
       " 'th nkh ch': 35213,\n",
       " 'nkh ch kh': 14542,\n",
       " 'sdl': 32639,\n",
       " 'ok': 18101,\n",
       " 'hope': 3894,\n",
       " 'sign': 33680,\n",
       " 'year': 48973,\n",
       " 'contract': 1583,\n",
       " 'start': 33990,\n",
       " 'chance': 1165,\n",
       " 'reliable': 31451,\n",
       " 'productive': 26741,\n",
       " 'time': 37118,\n",
       " 'hurt': 4023,\n",
       " 'playing': 23570,\n",
       " 'skills': 33737,\n",
       " 'falls': 2841,\n",
       " 'old': 18226,\n",
       " 'sdl ok': 32642,\n",
       " 'ok hope': 18125,\n",
       " 'year contract': 48992,\n",
       " 'reliable productive': 31452,\n",
       " 'productive time': 26748,\n",
       " 'time hurt': 37251,\n",
       " 'playing skills': 23596,\n",
       " 'old habits': 18289,\n",
       " 'sdl ok hope': 32643,\n",
       " 'ok hope sign': 18126,\n",
       " 'year contract start': 48993,\n",
       " 'reliable productive time': 31453,\n",
       " 'productive time hurt': 26749,\n",
       " 'time hurt playing': 37252,\n",
       " 'playing skills falls': 23597,\n",
       " 'yeah': 48880,\n",
       " 'shut': 33664,\n",
       " 'rest': 31842,\n",
       " 'faggot': 2811,\n",
       " 'friends': 3168,\n",
       " 'burned': 978,\n",
       " 'stake': 33973,\n",
       " 'shut fuck': 33665,\n",
       " 'burned stake': 979,\n",
       " 'fake': 2833,\n",
       " 'extremely': 2782,\n",
       " 'stupid': 34134,\n",
       " 'maybe': 6929,\n",
       " 'idiot': 4046,\n",
       " 'understands': 40669,\n",
       " 'health': 3729,\n",
       " 'understands taxation': 40670,\n",
       " 'women health': 46926,\n",
       " 'understands taxation women': 40671,\n",
       " 'wish': 46676,\n",
       " 'injury': 4209,\n",
       " 'happened': 3657,\n",
       " 'doubt': 2349,\n",
       " 'injured': 4208,\n",
       " 'looked': 5170,\n",
       " 'wish injury': 46695,\n",
       " 'wish injury happened': 46696,\n",
       " 'careful': 1077,\n",
       " 'og': 17924,\n",
       " 'fork': 3112,\n",
       " 'og fork': 17925,\n",
       " 'tonnyb': 37969,\n",
       " 'just': 4449,\n",
       " 'pay': 20662,\n",
       " 'attention': 430,\n",
       " 'tonnyb just': 37970,\n",
       " 'just don': 4463,\n",
       " 'don pay': 2288,\n",
       " 'pay attention': 20665,\n",
       " 'tonnyb just don': 37971,\n",
       " 'hmmm': 3834,\n",
       " 'pig': 22848,\n",
       " 'faced': 2795,\n",
       " 'laid': 4715,\n",
       " 'zero': 49887,\n",
       " 'getting': 3357,\n",
       " 'pregnant': 25617,\n",
       " 'hold': 3849,\n",
       " 'idk': 4062,\n",
       " 'stream': 34101,\n",
       " 'looking': 5172,\n",
       " 'pig faced': 22853,\n",
       " 'zero chance': 49888,\n",
       " 'pregnant activity': 25618,\n",
       " 'women idk': 46929,\n",
       " 'thought looking': 36661,\n",
       " 'pig faced laid': 22854,\n",
       " 'zero chance getting': 49890,\n",
       " 'pregnant activity hold': 25619,\n",
       " 'women idk stream': 46930,\n",
       " 'huh': 4001,\n",
       " 'income': 4156,\n",
       " 'spending': 33930,\n",
       " 'math': 6777,\n",
       " 'puts': 28171,\n",
       " 'math puts': 6786,\n",
       " 'reason': 30446,\n",
       " 'sound': 33877,\n",
       " 'retarded': 31852,\n",
       " 'lol': 5126,\n",
       " 'damn': 1794,\n",
       " 'negro': 12732,\n",
       " 'reason sound': 30504,\n",
       " 'reason sound retarded': 30505,\n",
       " 'racist': 28864,\n",
       " 'screen': 32515,\n",
       " 'nyou': 16843,\n",
       " 'pieceofshit': 22821,\n",
       " 'racist screen': 28925,\n",
       " 'screen nyou': 32534,\n",
       " 'nyou pieceofshit': 16942,\n",
       " 'racist screen nyou': 28927,\n",
       " 'screen nyou pieceofshit': 32535,\n",
       " 'oh': 17935,\n",
       " 'cheating': 1197,\n",
       " 'cup': 1761,\n",
       " 'cheat': 1196,\n",
       " 'fest': 2971,\n",
       " 'dare': 1808,\n",
       " 'post': 24825,\n",
       " 'comment': 1432,\n",
       " 'shame': 33560,\n",
       " 'oh cheating': 17948,\n",
       " 'post comment': 24841,\n",
       " 'oh cheating cup': 17949,\n",
       " 'post comment shame': 24845,\n",
       " 'dickhead': 2051,\n",
       " 'retard': 31850,\n",
       " 'head': 3721,\n",
       " 'post head': 24873,\n",
       " 'want': 43648,\n",
       " 'say': 32092,\n",
       " 'mike': 8373,\n",
       " 'want say': 43891,\n",
       " 'mike macwhogal': 8384,\n",
       " 'want say mike': 43892,\n",
       " 'http': 3943,\n",
       " 'www': 48458,\n",
       " 'youtube': 49772,\n",
       " 'com': 1397,\n",
       " 'watch': 44425,\n",
       " 'http www': 3983,\n",
       " 'www youtube': 48501,\n",
       " 'youtube com': 49773,\n",
       " 'com watch': 1408,\n",
       " 'http www youtube': 3984,\n",
       " 'www youtube com': 48502,\n",
       " 'youtube com watch': 49775,\n",
       " 'know': 4645,\n",
       " 've': 42097,\n",
       " 'holes': 3856,\n",
       " 'know ve': 4674,\n",
       " 've burned': 42100,\n",
       " 've burned holes': 42101,\n",
       " 'land': 4720,\n",
       " 'creature': 1709,\n",
       " 'chicken': 1213,\n",
       " 'claim': 1297,\n",
       " 'sky': 33745,\n",
       " 'falling': 2839,\n",
       " 'society': 33828,\n",
       " 'canada': 1043,\n",
       " 'marriage': 6410,\n",
       " 'nationwide': 11654,\n",
       " 'years': 49149,\n",
       " 'disaster': 2128,\n",
       " 'collapse': 1380,\n",
       " 'persecution': 22064,\n",
       " 'examples': 2731,\n",
       " 'starting': 33998,\n",
       " 'look': 5159,\n",
       " 'threats': 36818,\n",
       " 'warnings': 44220,\n",
       " 'hot': 3918,\n",
       " 'air': 150,\n",
       " 'gay marriage': 3319,\n",
       " 'marriage nationwide': 6475,\n",
       " 'nationwide years': 11655,\n",
       " 'years disaster': 49194,\n",
       " 'persecution shouting': 22069,\n",
       " 'look like': 5165,\n",
       " 'threats fears': 36819,\n",
       " 'warnings hot': 44221,\n",
       " 'marriage nationwide years': 6476,\n",
       " 'nationwide years disaster': 11656,\n",
       " 'years disaster collapse': 49195,\n",
       " 'persecution shouting examples': 22070,\n",
       " 'threats fears warnings': 36820,\n",
       " 'warnings hot air': 44222,\n",
       " 'craig': 1684,\n",
       " 'saying': 32259,\n",
       " 'entertaining': 2599,\n",
       " 'little': 5059,\n",
       " 'met': 8106,\n",
       " 'boyfriend': 874,\n",
       " 'season': 32714,\n",
       " 'thing': 35605,\n",
       " 'did': 2056,\n",
       " 'house': 3925,\n",
       " 'likes': 5019,\n",
       " 'day': 1828,\n",
       " 'sure': 34229,\n",
       " 'far': 2880,\n",
       " 'thanks': 35328,\n",
       " 'american': 215,\n",
       " 'horror': 3910,\n",
       " 'story': 34091,\n",
       " 'sweet': 34258,\n",
       " 'loves': 5219,\n",
       " 'saying start': 32345,\n",
       " 'met boyfriend': 8109,\n",
       " 'season thing': 32760,\n",
       " 'thing did': 35628,\n",
       " 'watch ahs': 44426,\n",
       " 'thanks american': 35331,\n",
       " 'saying start entertaining': 32346,\n",
       " 'met boyfriend season': 8110,\n",
       " 'season thing did': 32761,\n",
       " 'thing did watch': 35629,\n",
       " 'watch ahs house': 44427,\n",
       " 'thanks american horror': 35332,\n",
       " 'strange': 34095,\n",
       " 'talking': 34304,\n",
       " 'chelsea': 1203,\n",
       " 'fans': 2874,\n",
       " 'belittling': 616,\n",
       " 'community': 1465,\n",
       " 'actually': 74,\n",
       " 'key': 4579,\n",
       " 'cool': 1609,\n",
       " 'internet': 4262,\n",
       " 'points': 24046,\n",
       " 'collection': 1383,\n",
       " 'oh cool': 17952,\n",
       " 'points collection': 24053,\n",
       " 'oh cool internet': 17953,\n",
       " 'think': 35915,\n",
       " 'better': 651,\n",
       " 'coaches': 1366,\n",
       " 'glad': 3391,\n",
       " 'del': 1921,\n",
       " 'keeping': 4562,\n",
       " 'job': 4387,\n",
       " 'credit': 1714,\n",
       " 'took': 37986,\n",
       " 'average': 462,\n",
       " 'chicago': 1210,\n",
       " 'bulls': 966,\n",
       " 'teams': 34509,\n",
       " 'playoffs': 23615,\n",
       " 'row': 31993,\n",
       " 'think better': 35937,\n",
       " 'negro keeping': 12741,\n",
       " 'thought got': 36641,\n",
       " 'credit years': 1715,\n",
       " 'years took': 49320,\n",
       " 'took average': 37989,\n",
       " 'teams coached': 34512,\n",
       " 'playoffs years': 23624,\n",
       " 'years row': 49302,\n",
       " 'think better coaches': 35938,\n",
       " 'negro keeping job': 12742,\n",
       " 'thought got credit': 36642,\n",
       " 'years took average': 49321,\n",
       " 'took average chicago': 37990,\n",
       " 'teams coached playoffs': 34513,\n",
       " 'playoffs years row': 23625,\n",
       " 'nhmmmm': 13652,\n",
       " 'spin': 33937,\n",
       " 'obama': 17048,\n",
       " 'vietnam': 42591,\n",
       " 'troops': 39125,\n",
       " 'negative': 12692,\n",
       " 'answer': 266,\n",
       " 'doesn': 2218,\n",
       " 'matter': 6804,\n",
       " 'followers': 3079,\n",
       " 'll': 5089,\n",
       " 'nobama': 14926,\n",
       " 'team': 34391,\n",
       " 'make': 5288,\n",
       " 'liberal': 4869,\n",
       " 'messiah': 8083,\n",
       " 'light': 4911,\n",
       " 'leaking': 4784,\n",
       " 'classified': 1314,\n",
       " 'information': 4197,\n",
       " 'new': 12929,\n",
       " 'york': 49625,\n",
       " 'times': 37478,\n",
       " 'lets': 4849,\n",
       " 'photo': 22492,\n",
       " 'shoot': 33631,\n",
       " 'viet': 42588,\n",
       " 'nam': 11201,\n",
       " 'memorial': 7754,\n",
       " 'cares': 1080,\n",
       " 'vets': 42394,\n",
       " 'clear': 1318,\n",
       " 'immediate': 4115,\n",
       " 'area': 321,\n",
       " 'trusted': 39373,\n",
       " 'nhmmmm spin': 13653,\n",
       " 'obama honoring': 17162,\n",
       " 'vietnam troops': 42592,\n",
       " 'troops negative': 39135,\n",
       " 'negative answer': 12695,\n",
       " 'doesn matter': 2227,\n",
       " 'matter look': 6837,\n",
       " 'nobama team': 14945,\n",
       " 'team hmmm': 34436,\n",
       " 'make liberal': 5391,\n",
       " 'liberal messiah': 4871,\n",
       " 'messiah look': 8090,\n",
       " 'look better': 5162,\n",
       " 'better light': 654,\n",
       " 'classified information': 1315,\n",
       " 'new york': 13055,\n",
       " 'york times': 49638,\n",
       " 'times know': 37528,\n",
       " 'know lets': 4660,\n",
       " 'photo shoot': 22498,\n",
       " 'viet nam': 42589,\n",
       " 'nam memorial': 11202,\n",
       " 'memorial make': 7759,\n",
       " 'make obama': 5446,\n",
       " 'obama look': 17194,\n",
       " 'like cares': 4928,\n",
       " 'vets ll': 42397,\n",
       " 'vets immediate': 42395,\n",
       " 'nhmmmm spin obama': 13654,\n",
       " 'obama honoring vietnam': 17163,\n",
       " 'vietnam troops negative': 42593,\n",
       " 'troops negative answer': 39136,\n",
       " 'negative answer doesn': 12696,\n",
       " 'matter look followers': 6838,\n",
       " 'nobama team hmmm': 14946,\n",
       " 'team hmmm make': 34437,\n",
       " 'make liberal messiah': 5392,\n",
       " 'messiah look better': 8091,\n",
       " 'new york times': 13062,\n",
       " 'york times know': 49639,\n",
       " 'times know lets': 37529,\n",
       " 'photo shoot viet': 22499,\n",
       " 'viet nam memorial': 42590,\n",
       " 'nam memorial make': 11203,\n",
       " 'memorial make obama': 7760,\n",
       " 'make obama look': 5447,\n",
       " 'obama look like': 17195,\n",
       " 'vets ll clear': 42398,\n",
       " 'vets immediate area': 42396,\n",
       " 'good': 3445,\n",
       " 'article': 360,\n",
       " 'delivered': 1930,\n",
       " 'brick': 914,\n",
       " 'wall': 43583,\n",
       " 'warning': 44209,\n",
       " 'source': 33885,\n",
       " 'funding': 3266,\n",
       " 'media': 7439,\n",
       " 'matters': 6881,\n",
       " 'peter': 22315,\n",
       " 'lewis': 4859,\n",
       " 'progessive': 26867,\n",
       " 'insurance': 4239,\n",
       " 'company': 1469,\n",
       " 'auto': 453,\n",
       " 'run': 32011,\n",
       " 'minutes': 8824,\n",
       " 'cable': 1009,\n",
       " 'television': 34706,\n",
       " 'gal': 3284,\n",
       " 'looks': 5173,\n",
       " 'blow': 788,\n",
       " 'doll': 2246,\n",
       " 'red': 30841,\n",
       " 'lips': 5042,\n",
       " 'perfectly': 21925,\n",
       " 'round': 31989,\n",
       " 'mouth': 10332,\n",
       " 'mean': 7144,\n",
       " 'nif': 14062,\n",
       " 'progressive': 26931,\n",
       " 'premium': 25649,\n",
       " 'going': 3422,\n",
       " 'fund': 3264,\n",
       " 'hell': 3758,\n",
       " 'bent': 632,\n",
       " 'destroying': 2014,\n",
       " 'country': 1646,\n",
       " 'free': 3152,\n",
       " 'speech': 33920,\n",
       " 'mission': 9009,\n",
       " 'beck': 588,\n",
       " 'limbaugh': 5026,\n",
       " 'win': 46355,\n",
       " 'warning source': 44216,\n",
       " 'media matters': 7474,\n",
       " 'matters peter': 6891,\n",
       " 'peter lewis': 22320,\n",
       " 'progessive insurance': 26868,\n",
       " 'insurance company': 4240,\n",
       " 'minutes cable': 8829,\n",
       " 'television featuring': 34707,\n",
       " 'looks like': 5176,\n",
       " 'blow doll': 790,\n",
       " 'red lips': 30860,\n",
       " 'perfectly round': 21930,\n",
       " 'mouth know': 10363,\n",
       " 'know mean': 4665,\n",
       " 'mean nif': 7226,\n",
       " 'nif progressive': 14091,\n",
       " 'progressive insurance': 26936,\n",
       " 'just know': 4489,\n",
       " 'premium going': 25650,\n",
       " 'progressive liberal': 26942,\n",
       " 'hell bent': 3759,\n",
       " 'free speech': 3154,\n",
       " 'mission beck': 9010,\n",
       " 'warning source funding': 44217,\n",
       " 'media matters peter': 7478,\n",
       " 'matters peter lewis': 6892,\n",
       " 'peter lewis progessive': 22321,\n",
       " 'progessive insurance company': 26869,\n",
       " 'minutes cable television': 8830,\n",
       " 'television featuring gal': 34708,\n",
       " 'red lips perfectly': 30861,\n",
       " 'perfectly round mouth': 21931,\n",
       " 'mouth know mean': 10364,\n",
       " 'mean nif progressive': 7227,\n",
       " 'nif progressive insurance': 14092,\n",
       " 'progressive insurance just': 26937,\n",
       " 'premium going fund': 25651,\n",
       " 'progressive liberal hell': 26943,\n",
       " 'mission beck dobbs': 9011,\n",
       " 'sdnoriko': 32646,\n",
       " 'capcom': 1058,\n",
       " 'listens': 5052,\n",
       " 'love': 5212,\n",
       " 'okami': 18167,\n",
       " 'sdnoriko lol': 32647,\n",
       " 'watch fuck': 44449,\n",
       " 'wrong love': 48319,\n",
       " 'sdnoriko lol capcom': 32648,\n",
       " 'watch fuck hope': 44450,\n",
       " 'wrong love okami': 48320,\n",
       " 'sickening': 33674,\n",
       " 'ignorant': 4071,\n",
       " 'loser': 5185,\n",
       " 'fast': 2892,\n",
       " 'furious': 3272,\n",
       " 'didn': 2072,\n",
       " 'work': 47395,\n",
       " 'planned': 23322,\n",
       " 'didn work': 2084,\n",
       " 'work like': 47458,\n",
       " 'planned did': 23323,\n",
       " 'work like planned': 47459,\n",
       " 'planned did lololololololol': 23324,\n",
       " 'mitt': 9096,\n",
       " 'isn': 4313,\n",
       " 'best': 637,\n",
       " 'candidate': 1049,\n",
       " 'president': 25746,\n",
       " 'worst': 47904,\n",
       " 'help': 3764,\n",
       " 'tremendously': 38832,\n",
       " 'happen': 3656,\n",
       " 'way': 44653,\n",
       " 'mitt isn': 9105,\n",
       " 'isn best': 4314,\n",
       " 'candidate president': 1050,\n",
       " 'president going': 25795,\n",
       " 'worst help': 47913,\n",
       " 'tremendously nobama': 38833,\n",
       " 'nobama happen': 14931,\n",
       " 'mitt isn best': 9106,\n",
       " 'president going worst': 25796,\n",
       " 'worst help tremendously': 47914,\n",
       " 'tremendously nobama happen': 38834,\n",
       " 'nobama happen way': 14932,\n",
       " 'believe': 608,\n",
       " 'site': 33722,\n",
       " 'fools': 3090,\n",
       " 'useful': 41676,\n",
       " 'idiots': 4059,\n",
       " 'hiding': 3801,\n",
       " 'truth': 39383,\n",
       " 'protect': 27267,\n",
       " 'believe stupid': 611,\n",
       " 'stupid people': 34143,\n",
       " 'people site': 21683,\n",
       " 'useful idiots': 41681,\n",
       " 'truth fools': 39390,\n",
       " 'protect look': 27275,\n",
       " 'people site idea': 21684,\n",
       " 'useful idiots hiding': 41682,\n",
       " 'truth fools protect': 39391,\n",
       " 'protect look like': 27276,\n",
       " 'fucking': 3221,\n",
       " 'behave': 603,\n",
       " 'prick': 26095,\n",
       " 'relevant': 31433,\n",
       " 'fact': 2798,\n",
       " 'america': 212,\n",
       " 'relevant fact': 31434,\n",
       " 'relevant fact destroying': 31435,\n",
       " 'liar': 4865,\n",
       " 'sum': 34199,\n",
       " 'total': 38168,\n",
       " 'conversation': 1594,\n",
       " 'abuse': 28,\n",
       " 'calling': 1018,\n",
       " 'troll': 39000,\n",
       " 'typical': 40138,\n",
       " 'socialist': 33826,\n",
       " 'non': 15085,\n",
       " 'existent': 2753,\n",
       " 'class': 1308,\n",
       " 'warfare': 44169,\n",
       " 'pointless': 24041,\n",
       " 'total conversation': 38169,\n",
       " 'troll typical': 39051,\n",
       " 'typical socialist': 40159,\n",
       " 'non existent': 15120,\n",
       " 'class warfare': 1311,\n",
       " 'warfare pointless': 44176,\n",
       " 'total conversation abuse': 38170,\n",
       " 'troll typical socialist': 39052,\n",
       " 'typical socialist idiot': 40160,\n",
       " 'non existent class': 15122,\n",
       " 'welp': 45449,\n",
       " 'nbw': 11947,\n",
       " 'right': 31897,\n",
       " 'welp nbw': 45450,\n",
       " 'nbw right': 11948,\n",
       " 'welp nbw right': 45451,\n",
       " 'fellas': 2963,\n",
       " 'quite': 28589,\n",
       " 'read': 29624,\n",
       " 'rules': 32008,\n",
       " 'signature': 33681,\n",
       " 'piece': 22766,\n",
       " 'guess': 3572,\n",
       " 'great': 3535,\n",
       " 'blogs': 781,\n",
       " 'follow': 3077,\n",
       " 'share': 33585,\n",
       " 'commenting': 1443,\n",
       " 'throw': 36869,\n",
       " 'cents': 1140,\n",
       " 'add': 83,\n",
       " 'nhttps': 13762,\n",
       " 'plus': 23764,\n",
       " 'google': 3474,\n",
       " 'posts': 25056,\n",
       " 'congratulations': 1528,\n",
       " 'winners': 46539,\n",
       " 'submit': 34160,\n",
       " 'fun': 3260,\n",
       " 'good job': 3451,\n",
       " 'didn quite': 2078,\n",
       " 'quite read': 28622,\n",
       " 'read rules': 29728,\n",
       " 'piece guess': 22786,\n",
       " 'throw cents': 36879,\n",
       " 'post share': 24930,\n",
       " 'nhttps plus': 13763,\n",
       " 'plus google': 23775,\n",
       " 'google com': 3475,\n",
       " 'posts congratulations': 25059,\n",
       " 'winners hope': 46542,\n",
       " 'quite read rules': 28623,\n",
       " 'read rules signature': 29729,\n",
       " 'piece guess good': 22787,\n",
       " 'throw cents post': 36880,\n",
       " 'post share add': 24931,\n",
       " 'nhttps plus google': 13764,\n",
       " 'plus google com': 23776,\n",
       " 'posts congratulations winners': 25060,\n",
       " 'winners hope chance': 46543,\n",
       " 'hang': 3651,\n",
       " 'ass': 382,\n",
       " 'center': 1137,\n",
       " 'town': 38368,\n",
       " 'known': 4678,\n",
       " 'type': 40091,\n",
       " 'behavior': 604,\n",
       " 'gets': 3352,\n",
       " 'punishment': 27923,\n",
       " 'lady': 4713,\n",
       " 'kid': 4594,\n",
       " 'week': 45248,\n",
       " 'ago': 137,\n",
       " 'aren': 323,\n",
       " 'safe': 32041,\n",
       " 'sex': 33540,\n",
       " 'child': 1217,\n",
       " 'birth': 709,\n",
       " 'weeks': 45312,\n",
       " 'wayyyyy': 44992,\n",
       " 'soon': 33859,\n",
       " 'baby': 488,\n",
       " 'young': 49667,\n",
       " 'remember': 31582,\n",
       " 'event': 2705,\n",
       " 'happening': 3658,\n",
       " 'mother': 10140,\n",
       " 'recovers': 30816,\n",
       " 'quickly': 28547,\n",
       " 'say hang': 32149,\n",
       " 'town let': 38379,\n",
       " 'type behavior': 40094,\n",
       " 'type punishment': 40121,\n",
       " 'punishment lady': 27936,\n",
       " 'week ago': 45249,\n",
       " 'women aren': 46880,\n",
       " 'aren safe': 327,\n",
       " 'weeks wayyyyy': 45339,\n",
       " 'wayyyyy soon': 44993,\n",
       " 'young remember': 49705,\n",
       " 'remember event': 31597,\n",
       " 'hope mother': 3900,\n",
       " 'mother recovers': 10213,\n",
       " 'recovers quickly': 30817,\n",
       " 'say hang ass': 32150,\n",
       " 'town let known': 38380,\n",
       " 'type behavior gets': 40095,\n",
       " 'type punishment lady': 40122,\n",
       " 'punishment lady kid': 27937,\n",
       " 'week ago women': 45250,\n",
       " 'women aren safe': 46881,\n",
       " 'weeks wayyyyy soon': 45340,\n",
       " 'wayyyyy soon hurt': 44994,\n",
       " 'young remember event': 49706,\n",
       " 'remember event happening': 31598,\n",
       " 'mother recovers quickly': 10214,\n",
       " 'relaxe': 31403,\n",
       " 'result': 31847,\n",
       " 'perform': 21932,\n",
       " 'identity': 4041,\n",
       " 'relaxe better': 31404,\n",
       " 'perform identity': 21937,\n",
       " 'relaxe better result': 31405,\n",
       " 'perform identity work': 21938,\n",
       " 'abe': 6,\n",
       " 'delusion': 1932,\n",
       " 'weirdo': 45391,\n",
       " 'locked': 5117,\n",
       " 'inside': 4221,\n",
       " 'kept': 4575,\n",
       " 'away': 472,\n",
       " 'computers': 1499,\n",
       " 'doubts': 2351,\n",
       " 'lunacy': 5238,\n",
       " 'moronic': 10054,\n",
       " 'prog': 26862,\n",
       " 'nyc': 16822,\n",
       " 'thread': 36775,\n",
       " 'completely': 1488,\n",
       " 'come': 1413,\n",
       " 'apart': 285,\n",
       " 'seams': 32693,\n",
       " 'deep': 1885,\n",
       " 'end': 2560,\n",
       " 'weirdo locked': 45392,\n",
       " 'moronic prog': 10060,\n",
       " 'prog frog': 26863,\n",
       " 'nyc read': 16827,\n",
       " 'read thread': 29747,\n",
       " 'thread abe': 36776,\n",
       " 'seams deep': 32694,\n",
       " 'weirdo locked inside': 45393,\n",
       " 'moronic prog frog': 10061,\n",
       " 'prog frog nyc': 26864,\n",
       " 'nyc read thread': 16828,\n",
       " 'read thread abe': 29748,\n",
       " 'thread abe completely': 36777,\n",
       " 'seams deep end': 32695,\n",
       " 'clone': 1341,\n",
       " 'nbut': 11912,\n",
       " 'original': 19004,\n",
       " 'argument': 337,\n",
       " 'android': 237,\n",
       " 'existed': 2751,\n",
       " 'predates': 25523,\n",
       " 'ios': 4287,\n",
       " 'development': 2026,\n",
       " 'nbut original': 11939,\n",
       " 'original argument': 19005,\n",
       " 'predates ios': 25524,\n",
       " 'nbut original argument': 11940,\n",
       " 'original argument android': 19006,\n",
       " 'predates ios development': 25525,\n",
       " 'okc': 18197,\n",
       " 'deal': 1846,\n",
       " 'pg': 22377,\n",
       " 'plays': 23629,\n",
       " 'parker': 20021,\n",
       " 'nash': 11468,\n",
       " 'westbrook': 45594,\n",
       " 'unstoppable': 41139,\n",
       " 'imagine': 4104,\n",
       " 'shots': 33644,\n",
       " 'create': 1700,\n",
       " 'durant': 2444,\n",
       " 'natural': 11681,\n",
       " 'position': 24697,\n",
       " 'guy': 3602,\n",
       " 'okc work': 18206,\n",
       " 'work deal': 47419,\n",
       " 'pg plays': 22382,\n",
       " 'plays like': 23640,\n",
       " 'parker think': 20028,\n",
       " 'think nash': 36238,\n",
       " 'nash point': 11472,\n",
       " 'point westbrook': 24016,\n",
       " 'westbrook sg': 45601,\n",
       " 'team unstoppable': 34495,\n",
       " 'unstoppable imagine': 41140,\n",
       " 'nash create': 11469,\n",
       " 'westbrook natural': 45599,\n",
       " 'natural position': 11688,\n",
       " 'position guy': 24708,\n",
       " 'okc work deal': 18207,\n",
       " 'work deal pg': 47420,\n",
       " 'pg plays like': 22383,\n",
       " 'plays like parker': 23641,\n",
       " 'parker think nash': 20029,\n",
       " 'think nash point': 36239,\n",
       " 'nash point westbrook': 11473,\n",
       " 'point westbrook sg': 24017,\n",
       " 'westbrook sg team': 45602,\n",
       " 'team unstoppable imagine': 34496,\n",
       " 'unstoppable imagine shots': 41141,\n",
       " 'nash create durant': 11470,\n",
       " 'westbrook natural position': 45600,\n",
       " 'natural position guy': 11689,\n",
       " 'position guy doesn': 24709,\n",
       " 'maher': 5272,\n",
       " 'goes': 3420,\n",
       " 'npalin': 15489,\n",
       " 'nisn': 14391,\n",
       " 'market': 6379,\n",
       " 'wonderful': 47178,\n",
       " 'npalin alakda': 15490,\n",
       " 'nisn free': 14392,\n",
       " 'market wonderful': 6384,\n",
       " 'npalin alakda gets': 15491,\n",
       " 'nisn free market': 14393,\n",
       " 'pathetic': 20446,\n",
       " 'failure': 2824,\n",
       " 'troll pathetic': 39036,\n",
       " 'pathetic failure': 20463,\n",
       " 'troll pathetic failure': 39037,\n",
       " 'mille': 8482,\n",
       " 'sei': 33144,\n",
       " 'cos': 1633,\n",
       " 'nbaci': 11776,\n",
       " 'mille sabrina': 8483,\n",
       " 'sei cos': 33145,\n",
       " 'xec dolce': 48617,\n",
       " 'mille sabrina sei': 8484,\n",
       " 'sei cos xec': 33146,\n",
       " 'xec dolce nbaci': 48618,\n",
       " 'letting': 4853,\n",
       " 'sit': 33721,\n",
       " 'sticking': 34047,\n",
       " 'needle': 12611,\n",
       " 'veins': 42260,\n",
       " 'noose': 15222,\n",
       " 'choose': 1245,\n",
       " 'yeah better': 48887,\n",
       " 'years dime': 49192,\n",
       " 'needle veins': 12614,\n",
       " 'veins say': 42261,\n",
       " 'noose choose': 15223,\n",
       " 'yeah better letting': 48888,\n",
       " 'years dime sticking': 49193,\n",
       " 'needle veins say': 12615,\n",
       " 'veins say noose': 42262,\n",
       " 'noose choose kill': 15224,\n",
       " 'um': 40364,\n",
       " 'nholy': 13670,\n",
       " 'need': 12337,\n",
       " 'cigarette': 1274,\n",
       " 'um nholy': 40371,\n",
       " 'nholy fuck': 13671,\n",
       " 'need cigarette': 12364,\n",
       " 'um nholy fuck': 40372,\n",
       " 'nholy fuck need': 13672,\n",
       " 'moron': 9971,\n",
       " 'reach': 29594,\n",
       " 'moron truth': 10039,\n",
       " 'truth reach': 39432,\n",
       " 'moron truth reach': 10040,\n",
       " 'wistful': 46735,\n",
       " 'recovery': 30818,\n",
       " 'paul': 20588,\n",
       " 'raining': 29038,\n",
       " 'prayers': 25455,\n",
       " 'recovery paul': 30821,\n",
       " 'paul raining': 20621,\n",
       " 'raining prayers': 29039,\n",
       " 'recovery paul raining': 30822,\n",
       " 'paul raining prayers': 20622,\n",
       " 'laughs': 4747,\n",
       " 'heard': 3740,\n",
       " 'triggered': 38940,\n",
       " 'memories': 7764,\n",
       " 'high': 3802,\n",
       " 'flying': 3067,\n",
       " 'moving': 10504,\n",
       " 'triggered memories': 38943,\n",
       " 'memories high': 7765,\n",
       " 'moving beasts': 10509,\n",
       " 'triggered memories high': 38944,\n",
       " 'memories high flying': 7766,\n",
       " 'anti': 276,\n",
       " 'semitic': 33324,\n",
       " 'rants': 29259,\n",
       " 'welcomed': 45421,\n",
       " 'nfu': 13385,\n",
       " 'semitic rants': 33325,\n",
       " 'rants welcomed': 29262,\n",
       " 'welcomed racist': 45422,\n",
       " 'racist moron': 28910,\n",
       " 'moron nfu': 10016,\n",
       " 'semitic rants welcomed': 33326,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = count_vectorizer.transform(training_data['cleaned_comment'])\n",
    "y = training_data['Insult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3947x50000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 94859 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = [bool(np.random.binomial(1, .75)) for _ in range(X.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[np.array(mask)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[mask].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[~mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mAke this with sklearn \n",
    "def split_data(X, y, p=.75):\n",
    "    mask = np.array([bool(np.random.binomial(1, p)) for _ in range(X.shape[0])])\n",
    "    \n",
    "    X_train = X[mask]\n",
    "    y_train = y[mask]\n",
    "    X_validation = X[~mask]\n",
    "    y_validation = y[~mask]\n",
    "    \n",
    "    return X_train, y_train, X_validation, y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_validation, y_validation = split_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2941, 50000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteban.londono\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS_Platzi\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate model on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Score: 0.8131212723658051\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "predictions = model.predict(X_validation)\n",
    "validation_score = accuracy_score(y_validation, predictions)\n",
    "\n",
    "print('Validation Score:', validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_predictions = np.zeros(predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Score: 0.7037773359840954\n"
     ]
    }
   ],
   "source": [
    "baseline_validation_score = accuracy_score(y_validation, baseline_predictions)\n",
    "\n",
    "print('Validation Score:', baseline_validation_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember, everything is a hyper-parameter.. usalo copialo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionPipeline:\n",
    "    \n",
    "    def __init__(self, ngram_range, vectorizer_class, model_class, training_data):\n",
    "        self.ngram_range=ngram_range\n",
    "        self.vectorizer_class=vectorizer_class\n",
    "        self.model_class=model_class\n",
    "        self.training_data=training_data\n",
    "        self.vectorizer = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.model = None\n",
    "        self.validation_score = None\n",
    "        \n",
    "    def run(self):\n",
    "        self._fit_vectorizer()\n",
    "        self._featurize_text()\n",
    "        self._split_train_and_validation_sets()\n",
    "        self._fit_model_on_training_data()\n",
    "        self._validate_model_on_validation_set()\n",
    "        \n",
    "        print(\n",
    "            \"\"\"\n",
    "            Vectorizer Class: {vectorizer_class}\\n\\\n",
    "            N-gram Range: {ngram_range}\\n\\\n",
    "            Model Class: {model_class}\\n\\\n",
    "            Validation Score: {validation_score}\n",
    "            \"\"\".format(\n",
    "\n",
    "            vectorizer_class=repr(self.vectorizer_class.__name__), \n",
    "            ngram_range=self.ngram_range, \n",
    "            model_class=repr(self.model_class.__name__), \n",
    "            validation_score=round(self.validation_score, 4)\n",
    "\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _fit_vectorizer(self):\n",
    "        self.vectorizer = vectorizer_class(analyzer='word', ngram_range=ngram_range, \n",
    "                                     stop_words='english', max_features=50000)\n",
    "        self.vectorizer.fit(self.training_data['cleaned_comment'])\n",
    "    \n",
    "    def _featurize_text(self):\n",
    "        self.X = self.vectorizer.transform(self.training_data['cleaned_comment'])\n",
    "        self.y = self.training_data['Insult']\n",
    "\n",
    "    def _split_train_and_validation_sets(self):\n",
    "        self.X_train, self.y_train, self.X_validation, self.y_validation = split_data(\n",
    "            self.X, self.y)\n",
    "\n",
    "    def _fit_model_on_training_data(self):\n",
    "        self.model = self.model_class()\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def _validate_model_on_validation_set(self):\n",
    "        predictions = self.model.predict(self.X_validation)\n",
    "        self.validation_score = accuracy_score(self.y_validation, predictions)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Vectorizer Class: 'CountVectorizer'\n",
      "            N-gram Range: (1, 1)\n",
      "            Model Class: 'LogisticRegression'\n",
      "            Validation Score: 0.825\n",
      "            \n",
      "\n",
      "            Vectorizer Class: 'CountVectorizer'\n",
      "            N-gram Range: (1, 1)\n",
      "            Model Class: 'LinearSVC'\n",
      "            Validation Score: 0.7949\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteban.londono\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS_Platzi\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Vectorizer Class: 'CountVectorizer'\n",
      "            N-gram Range: (1, 1)\n",
      "            Model Class: 'RandomForestClassifier'\n",
      "            Validation Score: 0.8191\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteban.londono\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS_Platzi\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Vectorizer Class: 'TfidfVectorizer'\n",
      "            N-gram Range: (1, 1)\n",
      "            Model Class: 'LogisticRegression'\n",
      "            Validation Score: 0.799\n",
      "            \n",
      "\n",
      "            Vectorizer Class: 'TfidfVectorizer'\n",
      "            N-gram Range: (1, 1)\n",
      "            Model Class: 'LinearSVC'\n",
      "            Validation Score: 0.833\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteban.londono\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS_Platzi\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Vectorizer Class: 'TfidfVectorizer'\n",
      "            N-gram Range: (1, 1)\n",
      "            Model Class: 'RandomForestClassifier'\n",
      "            Validation Score: 0.7981\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteban.londono\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS_Platzi\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Vectorizer Class: 'CountVectorizer'\n",
      "            N-gram Range: (1, 2)\n",
      "            Model Class: 'LogisticRegression'\n",
      "            Validation Score: 0.8259\n",
      "            \n",
      "\n",
      "            Vectorizer Class: 'CountVectorizer'\n",
      "            N-gram Range: (1, 2)\n",
      "            Model Class: 'LinearSVC'\n",
      "            Validation Score: 0.8022\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteban.londono\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS_Platzi\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Vectorizer Class: 'CountVectorizer'\n",
      "            N-gram Range: (1, 2)\n",
      "            Model Class: 'RandomForestClassifier'\n",
      "            Validation Score: 0.8047\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteban.londono\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS_Platzi\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Vectorizer Class: 'TfidfVectorizer'\n",
      "            N-gram Range: (1, 2)\n",
      "            Model Class: 'LogisticRegression'\n",
      "            Validation Score: 0.7915\n",
      "            \n",
      "\n",
      "            Vectorizer Class: 'TfidfVectorizer'\n",
      "            N-gram Range: (1, 2)\n",
      "            Model Class: 'LinearSVC'\n",
      "            Validation Score: 0.8203\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteban.londono\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS_Platzi\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Vectorizer Class: 'TfidfVectorizer'\n",
      "            N-gram Range: (1, 2)\n",
      "            Model Class: 'RandomForestClassifier'\n",
      "            Validation Score: 0.7944\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteban.londono\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS_Platzi\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Vectorizer Class: 'CountVectorizer'\n",
      "            N-gram Range: (1, 3)\n",
      "            Model Class: 'LogisticRegression'\n",
      "            Validation Score: 0.8409\n",
      "            \n",
      "\n",
      "            Vectorizer Class: 'CountVectorizer'\n",
      "            N-gram Range: (1, 3)\n",
      "            Model Class: 'LinearSVC'\n",
      "            Validation Score: 0.811\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteban.londono\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS_Platzi\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Vectorizer Class: 'CountVectorizer'\n",
      "            N-gram Range: (1, 3)\n",
      "            Model Class: 'RandomForestClassifier'\n",
      "            Validation Score: 0.8093\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteban.londono\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS_Platzi\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Vectorizer Class: 'TfidfVectorizer'\n",
      "            N-gram Range: (1, 3)\n",
      "            Model Class: 'LogisticRegression'\n",
      "            Validation Score: 0.757\n",
      "            \n",
      "\n",
      "            Vectorizer Class: 'TfidfVectorizer'\n",
      "            N-gram Range: (1, 3)\n",
      "            Model Class: 'LinearSVC'\n",
      "            Validation Score: 0.8193\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteban.londono\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS_Platzi\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Vectorizer Class: 'TfidfVectorizer'\n",
      "            N-gram Range: (1, 3)\n",
      "            Model Class: 'RandomForestClassifier'\n",
      "            Validation Score: 0.772\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteban.londono\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS_Platzi\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Vectorizer Class: 'CountVectorizer'\n",
      "            N-gram Range: (1, 4)\n",
      "            Model Class: 'LogisticRegression'\n",
      "            Validation Score: 0.8221\n",
      "            \n",
      "\n",
      "            Vectorizer Class: 'CountVectorizer'\n",
      "            N-gram Range: (1, 4)\n",
      "            Model Class: 'LinearSVC'\n",
      "            Validation Score: 0.7943\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteban.londono\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS_Platzi\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Vectorizer Class: 'CountVectorizer'\n",
      "            N-gram Range: (1, 4)\n",
      "            Model Class: 'RandomForestClassifier'\n",
      "            Validation Score: 0.7965\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteban.londono\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS_Platzi\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Vectorizer Class: 'TfidfVectorizer'\n",
      "            N-gram Range: (1, 4)\n",
      "            Model Class: 'LogisticRegression'\n",
      "            Validation Score: 0.7677\n",
      "            \n",
      "\n",
      "            Vectorizer Class: 'TfidfVectorizer'\n",
      "            N-gram Range: (1, 4)\n",
      "            Model Class: 'LinearSVC'\n",
      "            Validation Score: 0.8103\n",
      "            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteban.londono\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS_Platzi\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Vectorizer Class: 'TfidfVectorizer'\n",
      "            N-gram Range: (1, 4)\n",
      "            Model Class: 'RandomForestClassifier'\n",
      "            Validation Score: 0.8025\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for ngram_range in [(1, 1), (1, 2), (1, 3), (1, 4)]:\n",
    "    for vectorizer_class in [CountVectorizer, TfidfVectorizer]:\n",
    "        for model_class in [LogisticRegression, LinearSVC, RandomForestClassifier]:\n",
    "            \n",
    "            # run prediction pipeline\n",
    "            prediction_pipeline = PredictionPipeline(\n",
    "                ngram_range=ngram_range,\n",
    "                vectorizer_class=vectorizer_class,\n",
    "                model_class=model_class,\n",
    "                training_data=training_data\n",
    "            )\n",
    "            \n",
    "            prediction_pipeline.run()\n",
    "            \n",
    "            # add hyper-parameters to `results` dictionary\n",
    "            results[str(prediction_pipeline.validation_score)] = {\n",
    "                    'vectorizer_class': prediction_pipeline.vectorizer_class,\n",
    "                    'ngram_range': prediction_pipeline.ngram_range,\n",
    "                    'model_class': prediction_pipeline.model_class\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8409090909090909\n",
      "Parameters: {'vectorizer_class': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'ngram_range': (1, 3), 'model_class': <class 'sklearn.linear_model.logistic.LogisticRegression'>}\n",
      "\n",
      "Score: 0.8329918032786885\n",
      "Parameters: {'vectorizer_class': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'ngram_range': (1, 1), 'model_class': <class 'sklearn.svm.classes.LinearSVC'>}\n",
      "\n",
      "Score: 0.8259149357072205\n",
      "Parameters: {'vectorizer_class': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'ngram_range': (1, 2), 'model_class': <class 'sklearn.linear_model.logistic.LogisticRegression'>}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_3_scores = sorted(results.keys(), reverse=True)[:3]\n",
    "\n",
    "for score in top_3_scores:\n",
    "    print('Score: {score}\\nParameters: {parameters}\\n'.format(\n",
    "        score=score, parameters=results[score]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_score_key = top_3_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteban.londono\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS_Platzi\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_class = results[top_score_key]['vectorizer_class']\n",
    "ngram_range = results[top_score_key]['ngram_range']\n",
    "model_class = results[top_score_key]['model_class']\n",
    "\n",
    "# fit vectorizer\n",
    "vectorizer = vectorizer_class(analyzer='word', ngram_range=ngram_range, stop_words='english', max_features=50000)\n",
    "vectorizer.fit(training_data['cleaned_comment'])\n",
    "\n",
    "# transform text\n",
    "X = vectorizer.transform(training_data['cleaned_comment'])\n",
    "y = training_data['Insult']\n",
    "\n",
    "# fit model on training data\n",
    "model = model_class()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a string: fuck you\n",
      "Insult?: False\n",
      "Please enter a string: you are stupid\n",
      "Insult?: True\n",
      "Please enter a string: eres marica\n",
      "Insult?: False\n",
      "Please enter a string: end\n",
      "Insult?: False\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    input_string = input('Please enter a string: ')\n",
    "    input_string = clean_text(input_string)\n",
    "    x_test = vectorizer.transform([input_string])\n",
    "    \n",
    "    prediction = model.predict(x_test)[0]\n",
    "    print('Insult?: {}'.format( bool(prediction)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
